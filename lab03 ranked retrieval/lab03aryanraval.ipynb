{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 03 - Ranked Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "import math\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No Stemming and Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your inverted index data structure\n",
    "inverted_index = {}\n",
    "\n",
    "# Variables to store document lengths and average document length\n",
    "doc_lengths = {}\n",
    "avg_doc_length = 0\n",
    "\n",
    "# Function to tokenize a document\n",
    "def tokenize(text):\n",
    "    # Use regular expression to tokenize words\n",
    "    return re.findall(r'\\b\\w+\\b', text.lower())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build the inverted index\n",
    "def build_inverted_index(doc_id, text):\n",
    "    # Tokenize the document\n",
    "    tokens = tokenize(text)\n",
    "    \n",
    "    # Update document length\n",
    "    doc_lengths[doc_id] = len(tokens)\n",
    "    \n",
    "    # Update average document length\n",
    "    global avg_doc_length\n",
    "    avg_doc_length = (avg_doc_length * (doc_id - 1) + len(tokens)) / doc_id\n",
    "    \n",
    "    # Build the inverted index\n",
    "    for term in set(tokens):\n",
    "        if term not in inverted_index:\n",
    "            inverted_index[term] = []\n",
    "        inverted_index[term].append((doc_id, tokens.count(term)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.idf similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate tf.idf similarity\n",
    "def tf_idf_similarity(query, doc_id):\n",
    "    similarity_score = 0\n",
    "\n",
    "    for term in set(query):\n",
    "        if term in inverted_index and doc_id in [doc[0] for doc in inverted_index[term]]:\n",
    "            tf_query = query.count(term)\n",
    "            tf_doc = next(doc[1] for doc in inverted_index[term] if doc[0] == doc_id)\n",
    "            idf = math.log(len(doc_lengths) / len(inverted_index[term]))\n",
    "            normalization = tf_doc + (2 * doc_lengths[doc_id] / avg_doc_length)\n",
    "            \n",
    "            similarity_score += (tf_query * tf_doc / normalization) * idf\n",
    "\n",
    "    return similarity_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run queries and return ranked results\n",
    "def run_queries(queries):\n",
    "    results = []\n",
    "\n",
    "    for query_id, query_text in queries.items():\n",
    "        query_tokens = tokenize(query_text)\n",
    "        query_results = []\n",
    "\n",
    "        for doc_id in doc_lengths.keys():\n",
    "            similarity_score = tf_idf_similarity(query_tokens, doc_id)\n",
    "            query_results.append((doc_id, similarity_score))\n",
    "\n",
    "        query_results.sort(key=lambda x: x[1], reverse=True)\n",
    "        results.extend([(query_id, doc[0], doc[1]) for doc in query_results])\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to write results to a file\n",
    "def write_results(results, filename):\n",
    "    with open(filename, 'w') as file:\n",
    "        for result in results:\n",
    "            file.write(','.join(map(str, result)) + '\\n')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing and Building Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse XML file and build the inverted index\n",
    "def parse_and_build_inverted_index(file_path):\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    for doc in root.findall('DOC'):\n",
    "        doc_id = int(doc.findtext('DOCNO'))\n",
    "        doc_text = doc.findtext('TEXT')\n",
    "        build_inverted_index(doc_id, doc_text)\n",
    "\n",
    "\n",
    "# Parse XML file and build the inverted index\n",
    "parse_and_build_inverted_index('trec.sample.xml')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Execution - No Stemming and Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read queries from file\n",
    "queries = {}\n",
    "with open('queries.lab3.txt', 'r') as query_file:\n",
    "    for line in query_file:\n",
    "        query_id, query_text = line.strip().split(' ', 1)\n",
    "        queries[int(query_id)] = query_text\n",
    "\n",
    "\n",
    "# Run queries for no stemming and stopping configurations\n",
    "results_no_stemming_stopping = run_queries(queries)\n",
    "write_results(results_no_stemming_stopping, 'tfidf.results')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization & Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your inverted index data structure\n",
    "inverted_index = {}\n",
    "\n",
    "# Variables to store document lengths and average document length\n",
    "doc_lengths = {}\n",
    "avg_doc_length = 0\n",
    "\n",
    "# Initialize the Porter Stemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "# Function to tokenize and stem a document\n",
    "def tokenize_and_stem(text):\n",
    "    # Use regular expression to tokenize words\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    # Apply stemming to each token\n",
    "    stemmed_tokens = [porter_stemmer.stem(token) for token in tokens]\n",
    "    return stemmed_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build the inverted index\n",
    "def build_inverted_index(doc_id, text):\n",
    "    # Tokenize and stem the document\n",
    "    tokens = tokenize_and_stem(text)\n",
    "    \n",
    "    # Update document length\n",
    "    doc_lengths[doc_id] = len(tokens)\n",
    "    \n",
    "    # Update average document length\n",
    "    global avg_doc_length\n",
    "    avg_doc_length = (avg_doc_length * (doc_id - 1) + len(tokens)) / doc_id\n",
    "    \n",
    "    # Build the inverted index\n",
    "    for term in set(tokens):\n",
    "        if term not in inverted_index:\n",
    "            inverted_index[term] = []\n",
    "        inverted_index[term].append((doc_id, tokens.count(term)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run queries and return ranked results\n",
    "def run_queries(queries):\n",
    "    results = []\n",
    "\n",
    "    for query_id, query_text in queries.items():\n",
    "        query_tokens = tokenize_and_stem(query_text)\n",
    "        query_results = []\n",
    "\n",
    "        for doc_id in doc_lengths.keys():\n",
    "            similarity_score = tf_idf_similarity(query_tokens, doc_id)\n",
    "            query_results.append((doc_id, similarity_score))\n",
    "\n",
    "        query_results.sort(key=lambda x: x[1], reverse=True)\n",
    "        results.extend([(query_id, doc[0], doc[1]) for doc in query_results])\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to write results to a file\n",
    "def write_results(results, filename):\n",
    "    with open(filename, 'w') as file:\n",
    "        for result in results:\n",
    "            file.write(','.join(map(str, result)) + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing and Building Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse XML file and build the inverted index\n",
    "def parse_and_build_inverted_index(file_path):\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    for doc in root.findall('DOC'):\n",
    "        doc_id = int(doc.findtext('DOCNO'))\n",
    "        doc_text = doc.findtext('TEXT')\n",
    "        build_inverted_index(doc_id, doc_text)\n",
    "\n",
    "\n",
    "# Parse XML file and build the inverted index\n",
    "parse_and_build_inverted_index('trec.sample.xml')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Execution - Stemming only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read queries from file\n",
    "queries = {}\n",
    "with open('queries.lab3.txt', 'r') as query_file:\n",
    "    for line in query_file:\n",
    "        query_id, query_text = line.strip().split(' ', 1)\n",
    "        queries[int(query_id)] = query_text\n",
    "\n",
    "\n",
    "# Run queries for different configurations\n",
    "results_stemming_only = run_queries(queries)\n",
    "write_results(results_stemming_only, 'tfidf.stem.results')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming and Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization, Stemming, & Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your inverted index data structure\n",
    "inverted_index = {}\n",
    "\n",
    "# Variables to store document lengths and average document length\n",
    "doc_lengths = {}\n",
    "avg_doc_length = 0\n",
    "\n",
    "# Initialize the Porter Stemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "# Define a list of stop words\n",
    "stop_words = [\"a\", \"an\", \"and\", \"the\", \"is\", \"of\", \"in\", \"to\", \"it\", \"for\", \"this\"]\n",
    "\n",
    "\n",
    "# Function to tokenize, stem, and remove stop words from a document\n",
    "def tokenize_stem_stop(text):\n",
    "    # Use regular expression to tokenize words\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    # Apply stemming to each token and remove stop words\n",
    "    tokens = [porter_stemmer.stem(token) for token in tokens if token not in stop_words]\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build the inverted index\n",
    "def build_inverted_index(doc_id, text):\n",
    "    # Tokenize, stem, and remove stop words from the document\n",
    "    tokens = tokenize_stem_stop(text)\n",
    "    \n",
    "    # Update document length\n",
    "    doc_lengths[doc_id] = len(tokens)\n",
    "    \n",
    "    # Update average document length\n",
    "    global avg_doc_length\n",
    "    avg_doc_length = (avg_doc_length * (doc_id - 1) + len(tokens)) / doc_id\n",
    "    \n",
    "    # Build the inverted index\n",
    "    for term in set(tokens):\n",
    "        if term not in inverted_index:\n",
    "            inverted_index[term] = []\n",
    "        inverted_index[term].append((doc_id, tokens.count(term)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run queries and return ranked results\n",
    "def run_queries(queries):\n",
    "    results = []\n",
    "\n",
    "    for query_id, query_text in queries.items():\n",
    "        query_tokens = tokenize_stem_stop(query_text)\n",
    "        query_results = []\n",
    "\n",
    "        for doc_id in doc_lengths.keys():\n",
    "            similarity_score = tf_idf_similarity(query_tokens, doc_id)\n",
    "            query_results.append((doc_id, similarity_score))\n",
    "\n",
    "        query_results.sort(key=lambda x: x[1], reverse=True)\n",
    "        results.extend([(query_id, doc[0], doc[1]) for doc in query_results])\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to write results to a file\n",
    "def write_results(results, filename):\n",
    "    with open(filename, 'w') as file:\n",
    "        for result in results:\n",
    "            file.write(','.join(map(str, result)) + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing and Building Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse XML file and build the inverted index\n",
    "def parse_and_build_inverted_index(file_path):\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    for doc in root.findall('DOC'):\n",
    "        doc_id = int(doc.findtext('DOCNO'))\n",
    "        doc_text = doc.findtext('TEXT')\n",
    "        build_inverted_index(doc_id, doc_text)\n",
    "\n",
    "\n",
    "# Parse XML file and build the inverted index\n",
    "parse_and_build_inverted_index('trec.sample.xml')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Execution - Stemming and Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read queries from file\n",
    "queries = {}\n",
    "with open('queries.lab3.txt', 'r') as query_file:\n",
    "    for line in query_file:\n",
    "        query_id, query_text = line.strip().split(' ', 1)\n",
    "        queries[int(query_id)] = query_text\n",
    "\n",
    "\n",
    "# Run queries for different configurations\n",
    "results_stemming_stopping = run_queries(queries)\n",
    "write_results(results_stemming_stopping, 'tfidf.all.results')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Details\n",
    "\n",
    "### Inverted Index from Lab 2\n",
    "\n",
    "In Lab 2, I implemented an inverted index by parsing an XML file, extracting relevant information (DOCNO and Text), and tokenizing the text for each document. The code for this was extended and adapted for the current lab.\n",
    "\n",
    "### tf.idf Retrieval Algorithm\n",
    "\n",
    "I implemented a tf.idf retrieval algorithm with the following key features:\n",
    "\n",
    "- Tokenization: I tokenized documents as in the previous assignment.\n",
    "- Stemming: I incorporated the Porter Stemmer from NLTK to apply stemming to both documents and queries.\n",
    "- Stopping: I introduced a stop words list and removed these common words from the tokenized and stemmed documents and queries.\n",
    "\n",
    "The tf.idf similarity function was employed to rank documents based on their relevance to the queries. I considered term frequency, document frequency, and document length normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Report\n",
    "\n",
    "### Challenges Faced\n",
    "\n",
    "Implementing the tf.idf algorithm with stemming and stopping posed challenges, especially in maintaining efficiency. The balance between precision and recall had to be carefully considered. Additionally, adapting the existing code from Lab 2 required a deep understanding of the tf.idf weighting scheme.\n",
    "\n",
    "### System Evaluation\n",
    "\n",
    "I ran the retrieval system for three configurations:\n",
    "\n",
    "- No stopping or stemming (tfidf.results)\n",
    "- Stemming only (tfidf.stem.results)\n",
    "- Stemming and stopping (tfidf.all.results)\n",
    "\n",
    "The results varied across configurations, reflecting the impact of stemming and stopping on the retrieval performance.\n",
    "\n",
    "### Efficiency Considerations\n",
    "\n",
    "Efficiency was a crucial aspect of the implementation. I utilized term-at-a-time evaluation and optimized storage of intermediate results to enhance efficiency. The use of an inverted index facilitated faster retrieval, especially for large collections.\n",
    "\n",
    "### Conclusion and Future Improvements\n",
    "\n",
    "Implementing a ranked retrieval system provided valuable insights into the importance of term weighting and the challenges of handling long documents. Future improvements could include parallelizing retrieval tasks for scalability and exploring advanced tokenization techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "The comparison of results across the three configurations revealed interesting patterns. Configuration 3 (stemming and stopping) exhibited improved precision, but at the expense of recall. This trade-off should be carefully considered based on the specific requirements of the retrieval system.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In conclusion, the lab allowed me to deepen my understanding of information retrieval concepts and gain practical experience in implementing a ranked retrieval system. The challenges faced provided valuable learning opportunities, and the results analysis highlighted the significance of preprocessing choices in retrieval tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
