{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\2533a\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\ufeffthe', 'project', 'gutenberg', 'ebook', 'king', 'jame', 'version', 'bibl', 'ebook', 'use', 'anyon', 'anywher', 'unit', 'state', 'part', 'world', 'cost', 'almost', 'restrict', 'whatsoev', 'may', 'copi', 'give', 'away', 'reus', 'term', 'project', 'gutenberg', 'licens', 'includ', 'ebook', 'onlin', 'wwwgutenbergorg', 'locat', 'unit', 'state', 'check', 'law', 'countri', 'locat', 'use', 'ebook', 'titl', 'king', 'jame', 'version', 'bibl', 'releas', 'date', 'august', '1', '1989', 'ebook', '10', 'recent', 'updat', 'may', '1', '2023', 'languag', 'english', 'start', 'project', 'gutenberg', 'ebook', 'king', 'jame', 'version', 'bibl', 'old', 'testament', 'king', 'jame', 'version', 'bibl', 'first', 'book', 'mose', 'call', 'genesi', 'second', 'book', 'mose', 'call', 'exodu', 'third', 'book', 'mose', 'call', 'leviticu', 'fourth', 'book', 'mose', 'call', 'number', 'fifth', 'book', 'mose', 'call', 'deuteronomi']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "text = \"\"\n",
    "\n",
    "for filename in ['bible.txt', 'quran.txt']:\n",
    "    with open(filename, 'r', encoding = 'utf-8') as file:\n",
    "        text += file.read() + ' '\n",
    "\n",
    "# Case folding\n",
    "text = text.lower()\n",
    "\n",
    "# Tokenization\n",
    "def tokenizer(text):\n",
    "    text = text.translate(str.maketrans('','',string.punctuation))\n",
    "    token = text.split()\n",
    "    return token\n",
    "\n",
    "tokens = tokenizer(text)\n",
    "\n",
    "# Stopping - Remove English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = []\n",
    "for word in tokens:\n",
    "    if word not in stop_words:\n",
    "        filtered_tokens.append(word)\n",
    "\n",
    "# Normalization - Perform Porter stemming\n",
    "porter = PorterStemmer()\n",
    "stemmed_tokens = []\n",
    "for word in filtered_tokens:\n",
    "    stemmed_tokens.append(porter.stem(word))\n",
    "    \n",
    "print(stemmed_tokens[:100])\n",
    "\n",
    "# Save preprocessed tokens to new files\n",
    "with open('bible_preprocessed.txt', 'w', encoding='utf-8') as bible_file:\n",
    "    bible_file.write(\" \".join(stemmed_tokens))\n",
    "\n",
    "with open('quran_preprocessed.txt', 'w', encoding='utf-8') as quran_file:\n",
    "    quran_file.write(\" \".join(stemmed_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the processed file to the new file. Are there any surprises? Discuss\n",
    "what kind of modifications in preprocessing could be applied. For example:\n",
    "- Additional words/terms to be filtered out\n",
    "- Special tokenization\n",
    "- Additional normalization to some terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
