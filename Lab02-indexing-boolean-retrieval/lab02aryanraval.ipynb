{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document ID: 1\n",
      "Profile: Profile 1\n",
      "Date: 2023-10-12\n",
      "Headline: Sample Headline\n",
      "Text: This is the sample text of the document.\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def parse_xml(xml_string):\n",
    "    # Parse the XML string\n",
    "    root = ET.fromstring(xml_string)\n",
    "\n",
    "    # Extract information from the XML elements\n",
    "    document_id = root.findtext('document_id')\n",
    "    profile = root.findtext('profile')\n",
    "    date = root.findtext('date')\n",
    "    headline = root.findtext('headline')\n",
    "    text = root.findtext('text')\n",
    "\n",
    "    # You can print or process the extracted information as needed\n",
    "    print(\"Document ID:\", document_id)\n",
    "    print(\"Profile:\", profile)\n",
    "    print(\"Date:\", date)\n",
    "    print(\"Headline:\", headline)\n",
    "    print(\"Text:\", text)\n",
    "\n",
    "# Example XML string\n",
    "sample_xml = \"\"\"\n",
    "<document>\n",
    "    <document_id>1</document_id>\n",
    "    <profile>Profile 1</profile>\n",
    "    <date>2023-10-12</date>\n",
    "    <headline>Sample Headline</headline>\n",
    "    <text>This is the sample text of the document.</text>\n",
    "</document>\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Call the parse_xml function with the sample XML string\n",
    "parse_xml(sample_xml)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing XML Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOCNO: 1\n",
      "Text: \n",
      "\t\tHe likes to wink, he likes to drink\n",
      "\t\n",
      "\n",
      "\n",
      "DOCNO: 2\n",
      "Text: \n",
      "\t\tHe likes to drink, and drink, and drink\n",
      "\t\n",
      "\n",
      "\n",
      "DOCNO: 3\n",
      "Text: \n",
      "\t\tThe thing he likes to drink is ink\n",
      "\t\n",
      "\n",
      "\n",
      "DOCNO: 4\n",
      "Text: \n",
      "\t\tThe ink he likes to drink is pink\n",
      "\t\n",
      "\n",
      "\n",
      "DOCNO: 5\n",
      "Text: \n",
      "\t\tHe likes to wink, and drink pink ink\n",
      "\t\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def parse_xml_file(file):\n",
    "    # Parse the XML file\n",
    "    tree = ET.parse(file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Iterate through each <DOC> element in the XML file\n",
    "    for doc in root.findall('DOC'):\n",
    "        # Extract information from the XML elements\n",
    "        docno = doc.findtext('DOCNO')\n",
    "        text = doc.findtext('Text')\n",
    "\n",
    "        # You can print or process the extracted information as needed\n",
    "        print(\"DOCNO:\", docno)\n",
    "        print(\"Text:\", text)\n",
    "        print(\"\\n\")\n",
    "\n",
    "\n",
    "# Open the XML file\n",
    "file_path = 'sample.xml'\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    # Call the parse_xml_file function with the file object\n",
    "    parse_xml_file(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headline Tokens: ['like', 'wink', ',', 'like', 'drink']\n",
      "Text Tokens: ['like', 'wink', ',', 'like', 'drink']\n",
      "\n",
      "\n",
      "Headline Tokens: ['like', 'drink', ',', 'drink', ',', 'drink']\n",
      "Text Tokens: ['like', 'drink', ',', 'drink', ',', 'drink']\n",
      "\n",
      "\n",
      "Headline Tokens: ['thing', 'like', 'drink', 'ink']\n",
      "Text Tokens: ['thing', 'like', 'drink', 'ink']\n",
      "\n",
      "\n",
      "Headline Tokens: ['ink', 'like', 'drink', 'pink']\n",
      "Text Tokens: ['ink', 'like', 'drink', 'pink']\n",
      "\n",
      "\n",
      "Headline Tokens: ['like', 'wink', ',', 'drink', 'pink', 'ink']\n",
      "Text Tokens: ['like', 'wink', ',', 'drink', 'pink', 'ink']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def tokenize_and_preprocess(text, use_stopping=True, use_stemming=True):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text.lower())  # Convert to lowercase for consistency\n",
    "\n",
    "    # Remove stopwords if stopping is enabled\n",
    "    if use_stopping:\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Apply stemming if stemming is enabled\n",
    "    if use_stemming:\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def parse_xml_file(file, use_stopping=True, use_stemming=True):\n",
    "    try:\n",
    "        root = ET.fromstring(file.read())\n",
    "\n",
    "        for document in root.findall('DOC'):\n",
    "            headline = document.findtext('Text')\n",
    "            text = document.findtext('Text')\n",
    "\n",
    "            # Tokenize and preprocess the headline and text\n",
    "            headline_tokens = tokenize_and_preprocess(headline, use_stopping, use_stemming)\n",
    "            text_tokens = tokenize_and_preprocess(text, use_stopping, use_stemming)\n",
    "\n",
    "            # You can print or process the tokens as needed\n",
    "            print(\"Headline Tokens:\", headline_tokens)\n",
    "            print(\"Text Tokens:\", text_tokens)\n",
    "            print(\"\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing XML content: {e}\")\n",
    "\n",
    "\n",
    "# Open the XML file\n",
    "file_path = 'sample.xml'\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    # Call the parse_xml_file function with the file object\n",
    "    parse_xml_file(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "like: [('1', 0), ('1', 3), ('1', 5), ('1', 8), ('2', 0), ('2', 6), ('3', 1), ('3', 5), ('4', 1), ('4', 5), ('5', 0), ('5', 6)]\n",
      "wink: [('1', 1), ('1', 6), ('5', 1), ('5', 7)]\n",
      ",: [('1', 2), ('1', 7), ('2', 2), ('2', 4), ('2', 8), ('2', 10), ('5', 2), ('5', 8)]\n",
      "drink: [('1', 4), ('1', 9), ('2', 1), ('2', 3), ('2', 5), ('2', 7), ('2', 9), ('2', 11), ('3', 2), ('3', 6), ('4', 2), ('4', 6), ('5', 3), ('5', 9)]\n",
      "thing: [('3', 0), ('3', 4)]\n",
      "ink: [('3', 3), ('3', 7), ('4', 0), ('4', 4), ('5', 5), ('5', 11)]\n",
      "pink: [('4', 3), ('4', 7), ('5', 4), ('5', 10)]\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import defaultdict\n",
    "\n",
    "def tokenize_and_preprocess(text, use_stopping=True, use_stemming=True):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    if use_stopping:\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    if use_stemming:\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def build_inverted_index(file, use_stopping=True, use_stemming=True):\n",
    "    inverted_index = defaultdict(list)\n",
    "\n",
    "    try:\n",
    "        tree = ET.fromstring(file.read())\n",
    "        root = tree\n",
    "\n",
    "        for doc in root.findall('DOC'):\n",
    "            document_id = doc.findtext('DOCNO')\n",
    "            headline = doc.findtext('Text')\n",
    "            text = doc.findtext('Text')\n",
    "\n",
    "            # Tokenize and preprocess the headline and text\n",
    "            headline_tokens = tokenize_and_preprocess(headline, use_stopping, use_stemming)\n",
    "            text_tokens = tokenize_and_preprocess(text, use_stopping, use_stemming)\n",
    "\n",
    "            # Update the inverted index with the document_id and positions\n",
    "            for position, token in enumerate(headline_tokens + text_tokens):\n",
    "                inverted_index[token].append((document_id, position))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error building inverted index: {e}\")\n",
    "\n",
    "    return inverted_index\n",
    "\n",
    "\n",
    "# Specify the correct path to your sample.xml file\n",
    "file_path = \"sample.xml\"\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    # Build the inverted index\n",
    "    inverted_index = build_inverted_index(file)\n",
    "\n",
    "\n",
    "# Print the inverted index for visualization\n",
    "for term, postings in inverted_index.items():\n",
    "    print(f\"{term}: {postings}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverted index has been written to trec.index.txt\n"
     ]
    }
   ],
   "source": [
    "def output_inverted_index(inverted_index, output_file_path):\n",
    "    try:\n",
    "        with open(output_file_path, 'w') as output_file:\n",
    "            for term, postings in sorted(inverted_index.items()):\n",
    "                posting_strings = [f\"{doc_id}:{pos}\" for doc_id, pos in postings]\n",
    "                output_line = f\"{term}: {', '.join(posting_strings)}\\n\"\n",
    "                output_file.write(output_line)\n",
    "\n",
    "        print(f\"Inverted index has been written to {output_file_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing inverted index to file: {e}\")\n",
    "\n",
    "\n",
    "# Specify the output file path\n",
    "output_file_path = \"trec.index.txt\"\n",
    "\n",
    "\n",
    "# Output the inverted index to a text file\n",
    "output_inverted_index(inverted_index, output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverted index has been loaded from trec.index.txt\n",
      "defaultdict(<class 'list'>, {'like': [('1', 0), ('1', 3), ('1', 5), ('1', 8), ('2', 0), ('2', 6), ('3', 1), ('3', 5), ('4', 1), ('4', 5), ('5', 0), ('5', 6)], 'wink': [('1', 1), ('1', 6), ('5', 1), ('5', 7)], ',': [('1', 2), ('1', 7), ('2', 2), ('2', 4), ('2', 8), ('2', 10), ('5', 2), ('5', 8)], 'drink': [('1', 4), ('1', 9), ('2', 1), ('2', 3), ('2', 5), ('2', 7), ('2', 9), ('2', 11), ('3', 2), ('3', 6), ('4', 2), ('4', 6), ('5', 3), ('5', 9)], 'thing': [('3', 0), ('3', 4)], 'ink': [('3', 3), ('3', 7), ('4', 0), ('4', 4), ('5', 5), ('5', 11)], 'pink': [('4', 3), ('4', 7), ('5', 4), ('5', 10)]})\n"
     ]
    }
   ],
   "source": [
    "def load_inverted_index(input_file_path):\n",
    "    inverted_index = defaultdict(list)\n",
    "\n",
    "    try:\n",
    "        with open(input_file_path, 'r') as input_file:\n",
    "            for line in input_file:\n",
    "                term, postings_str = line.strip().split(': ')\n",
    "                postings = [tuple(map(int, posting.split(':'))) for posting in postings_str.split(', ')]\n",
    "                inverted_index[term] = postings\n",
    "\n",
    "        print(f\"Inverted index has been loaded from {input_file_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading inverted index from file: {e}\")\n",
    "\n",
    "    return inverted_index\n",
    "\n",
    "\n",
    "# Specify the input file path\n",
    "input_file_path = \"trec.index.txt\"\n",
    "\n",
    "\n",
    "# Load the inverted index from the text file\n",
    "loaded_inverted_index = load_inverted_index(input_file_path)\n",
    "\n",
    "\n",
    "# Print Inverted Index\n",
    "print(inverted_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Overlap Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranked Documents:\n",
      "Document ID: 1, Similarity: 2.0\n",
      "Document ID: 5, Similarity: 2.0\n"
     ]
    }
   ],
   "source": [
    "def word_overlap_similarity(query_tokens, document_tokens):\n",
    "    intersection = set(query_tokens) & set(document_tokens)\n",
    "    union = set(query_tokens) | set(document_tokens)\n",
    "\n",
    "    similarity = len(intersection) / len(union) if len(union) > 0 else 0\n",
    "    return similarity\n",
    "\n",
    "def word_overlap_retrieval(query, inverted_index):\n",
    "    query_tokens = tokenize_and_preprocess(query)\n",
    "\n",
    "    # Calculate similarity for each document in the inverted index\n",
    "    document_similarities = {}\n",
    "    for term in query_tokens:\n",
    "        if term in inverted_index:\n",
    "            for document_id, _ in inverted_index[term]:\n",
    "                if document_id not in document_similarities:\n",
    "                    document_similarities[document_id] = 0\n",
    "                document_similarities[document_id] += 1\n",
    "\n",
    "    # Normalize the similarity scores by the length of the query\n",
    "    for document_id in document_similarities:\n",
    "        document_similarities[document_id] /= len(query_tokens)\n",
    "\n",
    "    # Sort documents by similarity score in descending order\n",
    "    ranked_documents = sorted(document_similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return ranked_documents\n",
    "\n",
    "\n",
    "# Example query\n",
    "query = \"wink\"\n",
    "\n",
    "\n",
    "# Perform word overlap retrieval\n",
    "ranked_documents = word_overlap_retrieval(query, loaded_inverted_index)\n",
    "\n",
    "\n",
    "# Print the ranked documents\n",
    "print(\"Ranked Documents:\")\n",
    "for document_id, similarity in ranked_documents:\n",
    "    print(f\"Document ID: {document_id}, Similarity: {similarity}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boolean Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result Documents: [1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "def boolean_search(query, inverted_index):\n",
    "    # Tokenize and preprocess the query\n",
    "    query_tokens = tokenize_and_preprocess(query)\n",
    "\n",
    "    # Handle AND, OR, and NOT operations\n",
    "    result_documents = set()\n",
    "\n",
    "    i = 0\n",
    "    while i < len(query_tokens):\n",
    "        term = query_tokens[i]\n",
    "\n",
    "        if term == \"AND\":\n",
    "            i += 1\n",
    "            next_term = query_tokens[i]\n",
    "            if next_term in inverted_index:\n",
    "                result_documents.intersection_update(set(doc_id for doc_id, _ in inverted_index[next_term]))\n",
    "            else:\n",
    "                result_documents.clear()\n",
    "\n",
    "        elif term == \"OR\":\n",
    "            i += 1\n",
    "            next_term = query_tokens[i]\n",
    "            if next_term in inverted_index:\n",
    "                result_documents.update(set(doc_id for doc_id, _ in inverted_index[next_term]))\n",
    "\n",
    "        elif term == \"NOT\":\n",
    "            i += 1\n",
    "            next_term = query_tokens[i]\n",
    "            if next_term in inverted_index:\n",
    "                result_documents.difference_update(set(doc_id for doc_id, _ in inverted_index[next_term]))\n",
    "\n",
    "        else:\n",
    "            if term in inverted_index:\n",
    "                result_documents.update(set(doc_id for doc_id, _ in inverted_index[term]))\n",
    "            else:\n",
    "                result_documents.clear()\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    # Convert the result to a list for better display\n",
    "    result_documents = list(result_documents)\n",
    "\n",
    "    return result_documents\n",
    "\n",
    "\n",
    "# Example Boolean query\n",
    "boolean_query = \"drink AND wink OR think NOT like\"\n",
    "\n",
    "\n",
    "# Perform Boolean search\n",
    "boolean_result = boolean_search(boolean_query, loaded_inverted_index)\n",
    "\n",
    "\n",
    "# Print the result documents\n",
    "print(\"Result Documents:\", boolean_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Overlap Retrieval:\n",
      "\n",
      "\n",
      "Boolean Search:\n",
      "Result Documents: []\n",
      "\n",
      "\n",
      "Analysis:\n",
      "Use Stopping: True, Use Stemming: True\n",
      "Word Overlap Result Documents: 0\n",
      "Boolean Search Result Documents: 0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def compare_and_analyze(query, inverted_index, use_stopping=True, use_stemming=True):\n",
    "    # Word Overlap Retrieval\n",
    "    print(\"Word Overlap Retrieval:\")\n",
    "    word_overlap_result = word_overlap_retrieval(query, inverted_index)\n",
    "    for document_id, similarity in word_overlap_result:\n",
    "        print(f\"Document ID: {document_id}, Similarity: {similarity}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Boolean Search\n",
    "    print(\"Boolean Search:\")\n",
    "    boolean_result = boolean_search(query, inverted_index)\n",
    "    print(\"Result Documents:\", boolean_result)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Analysis\n",
    "    print(\"Analysis:\")\n",
    "    print(f\"Use Stopping: {use_stopping}, Use Stemming: {use_stemming}\")\n",
    "    print(f\"Word Overlap Result Documents: {len(word_overlap_result)}\")\n",
    "    print(f\"Boolean Search Result Documents: {len(boolean_result)}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Example query\n",
    "query_to_compare = \"information retrieval\"\n",
    "\n",
    "# Perform comparison and analysis\n",
    "compare_and_analyze(query_to_compare, loaded_inverted_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
