{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document ID: 1\n",
      "Profile: Profile 1\n",
      "Date: 2023-10-12\n",
      "Headline: Sample Headline\n",
      "Text: This is the sample text of the document.\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def parse_xml(xml_string):\n",
    "    # Parse the XML string\n",
    "    root = ET.fromstring(xml_string)\n",
    "\n",
    "    # Extract information from the XML elements\n",
    "    document_id = root.findtext('document_id')\n",
    "    profile = root.findtext('profile')\n",
    "    date = root.findtext('date')\n",
    "    headline = root.findtext('headline')\n",
    "    text = root.findtext('text')\n",
    "\n",
    "    # You can print or process the extracted information as needed\n",
    "    print(\"Document ID:\", document_id)\n",
    "    print(\"Profile:\", profile)\n",
    "    print(\"Date:\", date)\n",
    "    print(\"Headline:\", headline)\n",
    "    print(\"Text:\", text)\n",
    "\n",
    "# Example XML string\n",
    "sample_xml = \"\"\"\n",
    "<document>\n",
    "    <document_id>1</document_id>\n",
    "    <profile>Profile 1</profile>\n",
    "    <date>2023-10-12</date>\n",
    "    <headline>Sample Headline</headline>\n",
    "    <text>This is the sample text of the document.</text>\n",
    "</document>\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Call the parse_xml function with the sample XML string\n",
    "parse_xml(sample_xml)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing XML Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOCNO: 1\n",
      "Text: \n",
      "\t\tHe likes to wink, he likes to drink\n",
      "\t\n",
      "\n",
      "\n",
      "DOCNO: 2\n",
      "Text: \n",
      "\t\tHe likes to drink, and drink, and drink\n",
      "\t\n",
      "\n",
      "\n",
      "DOCNO: 3\n",
      "Text: \n",
      "\t\tThe thing he likes to drink is ink\n",
      "\t\n",
      "\n",
      "\n",
      "DOCNO: 4\n",
      "Text: \n",
      "\t\tThe ink he likes to drink is pink\n",
      "\t\n",
      "\n",
      "\n",
      "DOCNO: 5\n",
      "Text: \n",
      "\t\tHe likes to wink, and drink pink ink\n",
      "\t\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def parse_xml_file(file):\n",
    "    # Parse the XML file\n",
    "    tree = ET.parse(file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Iterate through each <DOC> element in the XML file\n",
    "    for doc in root.findall('DOC'):\n",
    "        # Extract information from the XML elements\n",
    "        docno = doc.findtext('DOCNO')\n",
    "        text = doc.findtext('Text')\n",
    "\n",
    "        # You can print or process the extracted information as needed\n",
    "        print(\"DOCNO:\", docno)\n",
    "        print(\"Text:\", text)\n",
    "        print(\"\\n\")\n",
    "\n",
    "\n",
    "# Open the XML file\n",
    "file_path = 'sample.xml'\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    # Call the parse_xml_file function with the file object\n",
    "    parse_xml_file(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headline Tokens: ['like', 'wink', ',', 'like', 'drink']\n",
      "Text Tokens: ['like', 'wink', ',', 'like', 'drink']\n",
      "\n",
      "\n",
      "Headline Tokens: ['like', 'drink', ',', 'drink', ',', 'drink']\n",
      "Text Tokens: ['like', 'drink', ',', 'drink', ',', 'drink']\n",
      "\n",
      "\n",
      "Headline Tokens: ['thing', 'like', 'drink', 'ink']\n",
      "Text Tokens: ['thing', 'like', 'drink', 'ink']\n",
      "\n",
      "\n",
      "Headline Tokens: ['ink', 'like', 'drink', 'pink']\n",
      "Text Tokens: ['ink', 'like', 'drink', 'pink']\n",
      "\n",
      "\n",
      "Headline Tokens: ['like', 'wink', ',', 'drink', 'pink', 'ink']\n",
      "Text Tokens: ['like', 'wink', ',', 'drink', 'pink', 'ink']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def tokenize_and_preprocess(text, use_stopping=True, use_stemming=True):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text.lower())  # Convert to lowercase for consistency\n",
    "\n",
    "    # Remove stopwords if stopping is enabled\n",
    "    if use_stopping:\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Apply stemming if stemming is enabled\n",
    "    if use_stemming:\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def parse_xml_file(file, use_stopping=True, use_stemming=True):\n",
    "    try:\n",
    "        root = ET.fromstring(file.read())\n",
    "\n",
    "        for document in root.findall('DOC'):\n",
    "            headline = document.findtext('Text')\n",
    "            text = document.findtext('Text')\n",
    "\n",
    "            # Tokenize and preprocess the headline and text\n",
    "            headline_tokens = tokenize_and_preprocess(headline, use_stopping, use_stemming)\n",
    "            text_tokens = tokenize_and_preprocess(text, use_stopping, use_stemming)\n",
    "\n",
    "            # You can print or process the tokens as needed\n",
    "            print(\"Headline Tokens:\", headline_tokens)\n",
    "            print(\"Text Tokens:\", text_tokens)\n",
    "            print(\"\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing XML content: {e}\")\n",
    "\n",
    "\n",
    "# Open the XML file\n",
    "file_path = 'sample.xml'\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    # Call the parse_xml_file function with the file object\n",
    "    parse_xml_file(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "like: [('1', 0), ('1', 3), ('1', 5), ('1', 8), ('2', 0), ('2', 6), ('3', 1), ('3', 5), ('4', 1), ('4', 5), ('5', 0), ('5', 6)]\n",
      "wink: [('1', 1), ('1', 6), ('5', 1), ('5', 7)]\n",
      ",: [('1', 2), ('1', 7), ('2', 2), ('2', 4), ('2', 8), ('2', 10), ('5', 2), ('5', 8)]\n",
      "drink: [('1', 4), ('1', 9), ('2', 1), ('2', 3), ('2', 5), ('2', 7), ('2', 9), ('2', 11), ('3', 2), ('3', 6), ('4', 2), ('4', 6), ('5', 3), ('5', 9)]\n",
      "thing: [('3', 0), ('3', 4)]\n",
      "ink: [('3', 3), ('3', 7), ('4', 0), ('4', 4), ('5', 5), ('5', 11)]\n",
      "pink: [('4', 3), ('4', 7), ('5', 4), ('5', 10)]\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import defaultdict\n",
    "\n",
    "def tokenize_and_preprocess(text, use_stopping=True, use_stemming=True):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    if use_stopping:\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    if use_stemming:\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def build_inverted_index(file, use_stopping=True, use_stemming=True):\n",
    "    inverted_index = defaultdict(list)\n",
    "\n",
    "    try:\n",
    "        tree = ET.fromstring(file.read())\n",
    "        root = tree\n",
    "\n",
    "        for doc in root.findall('DOC'):\n",
    "            document_id = doc.findtext('DOCNO')\n",
    "            headline = doc.findtext('Text')\n",
    "            text = doc.findtext('Text')\n",
    "\n",
    "            # Tokenize and preprocess the headline and text\n",
    "            headline_tokens = tokenize_and_preprocess(headline, use_stopping, use_stemming)\n",
    "            text_tokens = tokenize_and_preprocess(text, use_stopping, use_stemming)\n",
    "\n",
    "            # Update the inverted index with the document_id and positions\n",
    "            for position, token in enumerate(headline_tokens + text_tokens):\n",
    "                inverted_index[token].append((document_id, position))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error building inverted index: {e}\")\n",
    "\n",
    "    return inverted_index\n",
    "\n",
    "\n",
    "# Specify the correct path to your sample.xml file\n",
    "file_path = \"sample.xml\"\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    # Build the inverted index\n",
    "    inverted_index = build_inverted_index(file)\n",
    "\n",
    "\n",
    "# Print the inverted index for visualization\n",
    "for term, postings in inverted_index.items():\n",
    "    print(f\"{term}: {postings}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverted index has been written to trec.index.txt\n"
     ]
    }
   ],
   "source": [
    "def output_inverted_index(inverted_index, output_file_path):\n",
    "    try:\n",
    "        with open(output_file_path, 'w') as output_file:\n",
    "            for term, postings in sorted(inverted_index.items()):\n",
    "                posting_strings = [f\"{doc_id}:{pos}\" for doc_id, pos in postings]\n",
    "                output_line = f\"{term}: {', '.join(posting_strings)}\\n\"\n",
    "                output_file.write(output_line)\n",
    "\n",
    "        print(f\"Inverted index has been written to {output_file_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing inverted index to file: {e}\")\n",
    "\n",
    "\n",
    "# Specify the output file path\n",
    "output_file_path = \"trec.index.txt\"\n",
    "\n",
    "\n",
    "# Output the inverted index to a text file\n",
    "output_inverted_index(inverted_index, output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverted index has been loaded from trec.index.txt\n",
      "defaultdict(<class 'list'>, {'like': [('1', 0), ('1', 3), ('1', 5), ('1', 8), ('2', 0), ('2', 6), ('3', 1), ('3', 5), ('4', 1), ('4', 5), ('5', 0), ('5', 6)], 'wink': [('1', 1), ('1', 6), ('5', 1), ('5', 7)], ',': [('1', 2), ('1', 7), ('2', 2), ('2', 4), ('2', 8), ('2', 10), ('5', 2), ('5', 8)], 'drink': [('1', 4), ('1', 9), ('2', 1), ('2', 3), ('2', 5), ('2', 7), ('2', 9), ('2', 11), ('3', 2), ('3', 6), ('4', 2), ('4', 6), ('5', 3), ('5', 9)], 'thing': [('3', 0), ('3', 4)], 'ink': [('3', 3), ('3', 7), ('4', 0), ('4', 4), ('5', 5), ('5', 11)], 'pink': [('4', 3), ('4', 7), ('5', 4), ('5', 10)]})\n"
     ]
    }
   ],
   "source": [
    "def load_inverted_index(input_file_path):\n",
    "    inverted_index = defaultdict(list)\n",
    "\n",
    "    try:\n",
    "        with open(input_file_path, 'r') as input_file:\n",
    "            for line in input_file:\n",
    "                term, postings_str = line.strip().split(': ')\n",
    "                postings = [tuple(map(int, posting.split(':'))) for posting in postings_str.split(', ')]\n",
    "                inverted_index[term] = postings\n",
    "\n",
    "        print(f\"Inverted index has been loaded from {input_file_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading inverted index from file: {e}\")\n",
    "\n",
    "    return inverted_index\n",
    "\n",
    "\n",
    "# Specify the input file path\n",
    "input_file_path = \"trec.index.txt\"\n",
    "\n",
    "\n",
    "# Load the inverted index from the text file\n",
    "loaded_inverted_index = load_inverted_index(input_file_path)\n",
    "\n",
    "\n",
    "# Print Inverted Index\n",
    "print(inverted_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Overlap Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranked Documents:\n",
      "Document ID: 1, Similarity: 2.0\n",
      "Document ID: 5, Similarity: 2.0\n"
     ]
    }
   ],
   "source": [
    "def word_overlap_similarity(query_tokens, document_tokens):\n",
    "    intersection = set(query_tokens) & set(document_tokens)\n",
    "    union = set(query_tokens) | set(document_tokens)\n",
    "\n",
    "    similarity = len(intersection) / len(union) if len(union) > 0 else 0\n",
    "    return similarity\n",
    "\n",
    "def word_overlap_retrieval(query, inverted_index):\n",
    "    query_tokens = tokenize_and_preprocess(query)\n",
    "\n",
    "    # Calculate similarity for each document in the inverted index\n",
    "    document_similarities = {}\n",
    "    for term in query_tokens:\n",
    "        if term in inverted_index:\n",
    "            for document_id, _ in inverted_index[term]:\n",
    "                if document_id not in document_similarities:\n",
    "                    document_similarities[document_id] = 0\n",
    "                document_similarities[document_id] += 1\n",
    "\n",
    "    # Normalize the similarity scores by the length of the query\n",
    "    for document_id in document_similarities:\n",
    "        document_similarities[document_id] /= len(query_tokens)\n",
    "\n",
    "    # Sort documents by similarity score in descending order\n",
    "    ranked_documents = sorted(document_similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return ranked_documents\n",
    "\n",
    "\n",
    "# Example query\n",
    "query = \"wink\"\n",
    "\n",
    "\n",
    "# Perform word overlap retrieval\n",
    "ranked_documents = word_overlap_retrieval(query, loaded_inverted_index)\n",
    "\n",
    "\n",
    "# Print the ranked documents\n",
    "print(\"Ranked Documents:\")\n",
    "for document_id, similarity in ranked_documents:\n",
    "    print(f\"Document ID: {document_id}, Similarity: {similarity}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boolean Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result Documents: [1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "def boolean_search(query, inverted_index):\n",
    "    # Tokenize and preprocess the query\n",
    "    query_tokens = tokenize_and_preprocess(query)\n",
    "\n",
    "    # Handle AND, OR, and NOT operations\n",
    "    result_documents = set()\n",
    "\n",
    "    i = 0\n",
    "    while i < len(query_tokens):\n",
    "        term = query_tokens[i]\n",
    "\n",
    "        if term == \"AND\":\n",
    "            i += 1\n",
    "            next_term = query_tokens[i]\n",
    "            if next_term in inverted_index:\n",
    "                result_documents.intersection_update(set(doc_id for doc_id, _ in inverted_index[next_term]))\n",
    "            else:\n",
    "                result_documents.clear()\n",
    "\n",
    "        elif term == \"OR\":\n",
    "            i += 1\n",
    "            next_term = query_tokens[i]\n",
    "            if next_term in inverted_index:\n",
    "                result_documents.update(set(doc_id for doc_id, _ in inverted_index[next_term]))\n",
    "\n",
    "        elif term == \"NOT\":\n",
    "            i += 1\n",
    "            next_term = query_tokens[i]\n",
    "            if next_term in inverted_index:\n",
    "                result_documents.difference_update(set(doc_id for doc_id, _ in inverted_index[next_term]))\n",
    "\n",
    "        else:\n",
    "            if term in inverted_index:\n",
    "                result_documents.update(set(doc_id for doc_id, _ in inverted_index[term]))\n",
    "            else:\n",
    "                result_documents.clear()\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    # Convert the result to a list for better display\n",
    "    result_documents = list(result_documents)\n",
    "\n",
    "    return result_documents\n",
    "\n",
    "\n",
    "# Example Boolean query\n",
    "boolean_query = \"drink AND wink OR think NOT like\"\n",
    "\n",
    "\n",
    "# Perform Boolean search\n",
    "boolean_result = boolean_search(boolean_query, loaded_inverted_index)\n",
    "\n",
    "\n",
    "# Print the result documents\n",
    "print(\"Result Documents:\", boolean_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Overlap Retrieval:\n",
      "\n",
      "\n",
      "Boolean Search:\n",
      "Result Documents: []\n",
      "\n",
      "\n",
      "Analysis:\n",
      "Use Stopping: True, Use Stemming: True\n",
      "Word Overlap Result Documents: 0\n",
      "Boolean Search Result Documents: 0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def compare_and_analyze(query, inverted_index, use_stopping=True, use_stemming=True):\n",
    "    # Word Overlap Retrieval\n",
    "    print(\"Word Overlap Retrieval:\")\n",
    "    word_overlap_result = word_overlap_retrieval(query, inverted_index)\n",
    "    for document_id, similarity in word_overlap_result:\n",
    "        print(f\"Document ID: {document_id}, Similarity: {similarity}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Boolean Search\n",
    "    print(\"Boolean Search:\")\n",
    "    boolean_result = boolean_search(query, inverted_index)\n",
    "    print(\"Result Documents:\", boolean_result)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Analysis\n",
    "    print(\"Analysis:\")\n",
    "    print(f\"Use Stopping: {use_stopping}, Use Stemming: {use_stemming}\")\n",
    "    print(f\"Word Overlap Result Documents: {len(word_overlap_result)}\")\n",
    "    print(f\"Boolean Search Result Documents: {len(boolean_result)}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Example query\n",
    "query_to_compare = \"information retrieval\"\n",
    "\n",
    "# Perform comparison and analysis\n",
    "compare_and_analyze(query_to_compare, loaded_inverted_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc: \n",
      "\t\tHe likes to wink, he likes to drink\n",
      "\t \n",
      "doc: \n",
      "\t\tHe likes to drink, and drink, and drink\n",
      "\t \n",
      "doc: \n",
      "\t\tThe thing he likes to drink is ink\n",
      "\t \n",
      "doc: \n",
      "\t\tThe ink he likes to drink is pink\n",
      "\t \n",
      "doc: \n",
      "\t\tHe likes to wink, and drink pink ink\n",
      "\t \n",
      "['he', 'likes', 'to', 'wink', 'he', 'likes', 'to', 'drink', 'he', 'likes', 'to', 'drink', 'and', 'drink', 'and', 'drink', 'the', 'thing', 'he', 'likes', 'to', 'drink', 'is', 'ink', 'the', 'ink', 'he', 'likes', 'to', 'drink', 'is', 'pink', 'he', 'likes', 'to', 'wink', 'and', 'drink', 'pink', 'ink']\n",
      "Word Frequencies: {'\\n': 10, '\\t': 15, 'H': 3, 'e': 14, ' ': 40, 'l': 6, 'i': 23, 'k': 20, 's': 8, 't': 7, 'o': 6, 'w': 2, 'n': 18, ',': 4, 'h': 6, 'd': 10, 'r': 7, 'a': 3, 'T': 2, 'g': 1, 'p': 2}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'doclist'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 130\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# Update word frequencies and count of unique words\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m#countFreq(collectionText)\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \n\u001b[0;32m    128\u001b[0m \u001b[38;5;66;03m# Display results\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWord Frequencies:\u001b[39m\u001b[38;5;124m\"\u001b[39m, freq)\n\u001b[1;32m--> 130\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInverted Index:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43minvertedList\u001b[49m\u001b[43m(\u001b[49m\u001b[43minvertedIndex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollectionText\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[1], line 49\u001b[0m, in \u001b[0;36minvertedList\u001b[1;34m(invertedIndex, wordList, docNumber)\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[38;5;66;03m#{\"df\":1,} for every term need df for each of docs\u001b[39;00m\n\u001b[0;32m     48\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m docNumber \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m invertedIndex[term][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocList\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m---> 49\u001b[0m             \u001b[43minvertedIndex\u001b[49m\u001b[43m[\u001b[49m\u001b[43mterm\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdoclist\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mappend(docNumber)\n\u001b[0;32m     50\u001b[0m             invertedIndex[term][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdf\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m invertedIndex\n",
      "\u001b[1;31mKeyError\u001b[0m: 'doclist'"
     ]
    }
   ],
   "source": [
    "\n",
    "#Tokenizing function convert text into tokens with no punctuation\n",
    "\n",
    "# read file and create tokens between whitespace\n",
    "# import the bs module for reading xml text and nltk and string\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "### replace /home/jupyter-lballest@mtholyoke-8f27d with the name of your home directory\n",
    "### you should only need to replace the part containing the username (e.g. lballest to your username)\n",
    "file = 'sample.xml'\n",
    "\n",
    "\n",
    "\n",
    "# Initialize global variables\n",
    "wordCount = 0\n",
    "uniqueCount = 0\n",
    "freq = {}\n",
    "uniqueWords = []\n",
    "tokenList = []\n",
    "invertedIndex = {}  # Inverted index to store term information\n",
    "docList = [] #list that keeps track of the documents in which the term occurs\n",
    "def preProcessing(x):\n",
    "    #remove puntuation by creating translation table that specifies items to be replaced in a String\n",
    "    punctTable = str.maketrans('','',string.punctuation)\n",
    "    x = x.translate(punctTable)\n",
    "    # convert words to lowercase\n",
    "    x = x.lower()\n",
    "    token = x.split()\n",
    "    return token\n",
    "#count the frequency of each word in wordList update unique word count\n",
    "def countFreq(wordList):\n",
    "    global uniqueCount, freq\n",
    "    for word in wordList:\n",
    "        freq[word] = freq.get(word,0)+ 1\n",
    "        if freq[word] == 1:\n",
    "            uniqueCount += 1\n",
    "        uniqueWords.append(uniqueCount)\n",
    "    return wordList\n",
    "def invertedList(invertedIndex,wordList,docNumber):\n",
    "    for word in wordList:\n",
    "        for term in word:\n",
    "            # Update the inverted index\n",
    "            if term not in invertedIndex:\n",
    "                invertedIndex[term] = {\"df\": 1, \"docList\": []}\n",
    "                #{\"df\":1,} for every term need df for each of docs\n",
    "               \n",
    "            if docNumber not in invertedIndex[term][\"docList\"]:\n",
    "                invertedIndex[term][\"doclist\"].append(docNumber)\n",
    "                invertedIndex[term][\"df\"] += 1\n",
    "    return invertedIndex    \n",
    "def stopWords(wordList):\n",
    "    s = open(\"stoplist.txt\", \"r\")\n",
    "    for word in wordList:\n",
    "        if word in s:\n",
    "          #remove word\n",
    "           wordList.remove(word)\n",
    "    return wordList\n",
    "\n",
    "def wordStemmer(wordList):\n",
    "    # Initialize porter stemmer\n",
    "    ps = PorterStemmer()\n",
    "    for word in wordList:        \n",
    "         (\"{0:20}{1:20}\".format(word, ps.stem(word)))\n",
    "    return wordList  \n",
    "\n",
    "# read from the xml documents\n",
    "#xml_file = input(\"Enter the name of the XML file: \")\n",
    "#outputFileName = input(\"Enter the name of the output file to record the tokenList to: \")\n",
    "\n",
    "#open xml file\n",
    "fp = open('sample.xml','r')\n",
    "#outputFileName = input(\"Enter the name of the output file to record the invertedList to: \")\n",
    "\n",
    "\n",
    "#read line by line\n",
    "text = fp.read()\n",
    "\n",
    "# parse with html\n",
    "soup = BeautifulSoup(text, 'html.parser')\n",
    "\n",
    "# list to collect the list of tokens for each doc\n",
    "collectionText = []\n",
    "\n",
    "docNum = 0  # Start with doc_id 0\n",
    "# iterate on the 'HEADLINE' nodes of the tree\n",
    "for doc in soup('doc'):\n",
    "    docNum += 1\n",
    "    # initialize string to collect document headlines and text\n",
    "    text = \"\"\n",
    "    # get the headline nodes of each doc tag and save the text associated with that tag\n",
    "    # iterate on each headline\n",
    "    for head in doc.find_all('DOC'):\n",
    "        text += head.text\n",
    "        text += \" \"\n",
    "    for body in doc.find_all('text'):\n",
    "        text += body.text\n",
    "        text += ' '\n",
    "       \n",
    "    # Update total word count for the text\n",
    "    wordCount += len(text)\n",
    "    # Count word frequencies and update the inverted index\n",
    "    countFreq(text)\n",
    "   \n",
    "    #display the document before processing\n",
    "    print(\"doc:\",text)\n",
    "     #tokenize and normalize doc text\n",
    "    preProcessed = preProcessing(text)\n",
    "   \n",
    "    # append doc text\n",
    "    collectionText.extend(preProcessed)\n",
    "   \n",
    "print(collectionText)\n",
    "# Output the inverted index to a text file\n",
    "with open(\"inverted_index.txt\", \"w\") as output_file:\n",
    "    for term, data in invertedIndex.items():\n",
    "        output_file.write(\"Term: {term}\")\n",
    "        output_file.write(\"Document Frequency (df): {data['df']}\")\n",
    "        output_file.write(\"Documents where the term occurred:\")\n",
    "        for doc_id in data[\"docNum\"]:\n",
    "            output_file.write(f\"Document{docNum}\")\n",
    "# Update word frequencies and count of unique words\n",
    "#countFreq(collectionText)\n",
    "\n",
    "#notes\n",
    "#df needs to be in a dictionary\n",
    "\n",
    "# Display results\n",
    "print(\"Word Frequencies:\", freq)\n",
    "print(\"Inverted Index:\", invertedList(invertedIndex, collectionText, 0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
